Дорожная карта разработки Go2Asia: поэтапный план до финального релиза

Описание: Ниже представлен подробный план разработки платформы Go2Asia, разделённый на фазы и этапы. Каждая фаза включает цели, набор задач, ожидаемые результаты, задействованные модули/сервисы и роли мультиагентной команды. В соответствии с новой стратегией приоритет отдан контентным модулям (Atlas, Pulse, Blog) на этапе MVP, остальные функциональные компоненты подключаются на следующих фазах. Разработка ведётся мультиагентной командой ИИ: проектный Оркестратор делит работу на этапы и координирует специализированных агентов (DevOps, архитектор, фронтенд, бэкенд, тестировщик и др.), что позволяет параллельно реализовывать различные части проекта без перегрузки одним контекстом. План охватывает все шаги от закладки инфраструктуры до оптимизаций перед финальным релизом.

# Фаза 0: Архитектурное основание (монорепозиторий и инфраструктура)

Цель фазы 0: Заложить прочный фундамент проекта перед разработкой пользовательских функций. Это включает настройку репозитория, CI/CD, облачной инфраструктуры, базовой архитектуры фронтенда и SSO-аутентификации. К концу фазы команда получает единое централизованное окружение, готовое для быстрого запуска функциональных модулей.

## Этап 0.1: Инициализация проекта и мультиагентной команды.

•	Цель: Определить архитектурное видение и организовать процесс разработки с участием нескольких AI-агентов.
•	Задействованные компоненты: Документация по архитектуре; репозиторий; мультиагентные роли.
•	Вовлечённые агенты: ИИ-архитектор, ИИ-планировщик, ИИ-DevOps-инженер, ИИ-менеджер проекта (оркестратор).
•	Задачи:
o	Разработать первоначальную архитектурную концепцию системы: определить основные модули фронтенда и сервисы бэкенда, выбрать технологический стек, наметить структуру базы данных и интеграции.
o	Спланировать этапы реализации (декомпозиция на фазы и подэтапы) с приоритизацией контентных модулей. Оркестратор разбивает проект на задачи и назначает ответственных субагентов по ролям.
o	Сформировать мультиагентную среду разработки: подготовить системные промпты для каждой роли (проектный менеджер, аналитик, архитектор, разработчики, тестировщик и др.) в репозитории, настроить координацию между агентами через оркестратор.
•	Результат: Чёткий план работ на все фазы, структурированная команда ИИ-агентов с разграниченными зонами ответственности. Проект готов к одновременному старту нескольких направлений разработки (инфраструктура, фронтенд, бэкенд) под управлением оркестратора.

## Этап 0.2: Настройка монорепозитория.

•	Цель: Создать единый монорепозиторий go2asia/ с продуманной структурой каталогов для всех частей системы.
•	Задействованные компоненты: Репозиторий (структура каталогов apps/, services/, packages/, infra/).
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-архитектор.
•	Задачи:
o	Инициализировать Git-репозиторий и базовую файловую структуру: директории apps/ (микрофронтенд PWA-модули – Atlas, Blog и др.), services/ (бэкенд-микросервисы – Auth, Content и т.д.), packages/ (общие библиотеки, контракты API/SDK) и infra/ (инфраструктурные конфигурации, скрипты).
o	Описать README и базовую документацию проекта, зафиксировать принятые конвенции (например, кодстайл, соглашения по коммитам) для согласованной работы всех агентов-разработчиков.
o	Добавить шаблоны модулей и сервисов: подготовить пустые PWA-приложения и шаблонные сервисы (Cursor) в соответствующих каталогах, чтобы унифицировать старт разработки для субагентов фронтенда и бэкенда.
•	Результат: Монорепозиторий инициализирован и опубликован. Структура понятна всем участникам и облегчает совместную разработку и повторное использование кода. Команда имеет единое место для работы над всеми частями экосистемы.

## Этап 0.3: CI/CD и окружение разработки.

•	Цель: Автоматизировать сборку, тестирование и развёртывание кода для ускорения итераций разработки.
•	Задействованные компоненты: Конвейер CI/CD (GitHub Actions), Docker-окружение, ORM для БД.
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-архитектор, ИИ-тестировщик.
•	Задачи:
o	Настроить контейнеризацию: написать Dockerfile и/или docker-compose для ключевых сервисов, обеспечить единообразное локальное окружение. Бэкенд-сервисы упаковываются в Docker-образы, запускаемые локально и в облаке, а также подключается ORM (например, Prisma/Drizzle) с начальными миграциями БД.
o	Внедрить CI/CD: сконфигурировать GitHub Actions для автосборки и тестирования каждого pull request, а при слиянии в main – автоматического деплоя. Настроить процесс Preview Deployments: каждую feature-ветку деплоить во временное окружение (например, на Netlify) для обозрения результатов до слияния.
o	Настроить базовые тесты и линтинг: подключить инструмент юнит-тестирования и линтер/форматтер, добавить запуск тестов в CI. QA-агент пишет первые простые автотесты на критичные функции (например, отклик API).
•	Результат: Непрерывная интеграция и поставка настроены. Сборки фронтенда и бэкенда проходят автоматически, тесты запускаются, образы сервисов публикуются. Разработчики могут локально запускать все сервисы в Docker. Это обеспечивает быстрые итерации и контроль качества на ранней стадии.

## Этап 0.4: Облачная инфраструктура и сеть.

•	Цель: Подготовить удалённую инфраструктуру для хостинга приложения, доставки контента и защиты.
•	Задействованные компоненты: Домен и DNS, Cloudflare CDN/WAF, облачное хранилище.
•	Вовлечённые агенты: ИИ-DevOps-инженер.
•	Задачи:
o	Зарегистрировать основной домен (например, go2asia.space) и настроить DNS через Cloudflare. Включить Cloudflare CDN для раздачи статики и Web Application Firewall (WAF) для защиты от DDoS и веб-атак.
o	Подключить облачное хранилище (Cloudflare R2) для пользовательских медиафайлов – аватаров, фотографий и пр. Настроить сервис хранения так, чтобы все модули могли использовать его через API или прямые ссылки.
o	Заложить возможность Edge-вычислений: инициализировать пустые Cloudflare Workers (серверлесс-функции), которые пока не выполняют логику, но интегрированы в проект. В следующих фазах они будут использоваться для выноса ресурсоёмких задач ближе к пользователю.
•	Результат: Платформа зарегистрирована в сети под собственным доменом. Сеть доставки контента (CDN) и базовые облачные сервисы готовы. Проект имеет защищённый периметр (WAF) и место для хранения пользовательского контента. Инфраструктура масштабируема: дальнейшие сервисы и фронтенды будут развертываться на этой основе.

## Этап 0.5: Единая аутентификация (SSO).

•	Цель: Внедрить централизованную систему управления пользователями и входом, общую для всех модулей.
•	Задействованные компоненты: SSO-сервис (Clerk), Auth-модуль, база пользователей.
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-бэкенд-разработчик (Auth).
•	Задачи:
o	Интегрировать облачный SSO-провайдер Clerk: зарегистрировать приложение Go2Asia в Clerk, настроить redirect URI (например, auth.go2asia.space), подключить домены. Проверить, что Clerk может устанавливать JWT-cookies на *.go2asia.space для SSO между субдоменами.
o	Определить роли пользователей и хранение профилей: использовать возможности Clerk для базовых атрибутов (имя, email) и ролей. Задать в профиле пользователя флаги ролей (обычный пользователь Spacer, VIP, PRO-куратор, партнёр, админ). Пока роли просто сохраняются в метаданных профиля, в будущем они будут влиять на доступ к функциям.
o	Обеспечить доступ всех микрофронтендов к SSO: настроить в каждом фронтенд-модуле использование Clerk SDK. При входе пользователя через любой модуль сессия должна признаваться остальными (cookie на основном домене).
•	Результат: Все части платформы используют общую базу пользователей. Реализован единый вход: пользователь авторизуется один раз и получает доступ ко всем сервисам. На данном этапе логика авторизации минимальна (Clerk обеспечивает OAuth, проверку email, хранение пользователей), дальнейшее управление правами будет добавлено в следующих фазах.

## Этап 0.6: Каркас PWA-оболочки и микрофронтендов.

•	Цель: Спроектировать и реализовать базовый App Shell – контейнер для модульного фронтенда, и определить стратегию подключения отдельных PWA-модулей.
•	Задействованные компоненты: Главный фронтенд (оболочка), маршрутизация, сервис-воркер.
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-архитектор.
•	Задачи:
o	Создать App Shell – главное PWA-приложение (например, на React + Vite или Next.js), развёрнутое на основном домене. App Shell отвечает за общий интерфейс: шапку, меню навигации, контейнер для встраивания модулей, а также за регистрацию сервис-воркера PWA.
o	Настроить маршрутизацию под микрофронтенды: определить URL для каждого раздела (/atlas, /pulse, /blog и т.д.). При переходе на эти пути оболочка должна динамически загружать соответствующий модуль интерфейса. Реализовать ленивую загрузку бандлов с помощью Module Federation или ESM import maps.
o	Внедрить mobile-first подход и офлайн-режим по умолчанию: включить PWA Manifest и базовый Service Worker (сгенерированный, напр., Workbox) с кешированием основных файлов. Оболочка должна показывать заглушку “Offline”, если сеть недоступна, и обеспечить адаптивность UI для мобильных устройств.
•	Результат: Главный фронтенд-приложение (App Shell) запущено и доступно по основному домену. Пока оно содержит минимальный UI и заглушки разделов, но уже умеет подгружать внешние модули. Базовая навигация и PWA-функции (офлайн поддержка, иконки, манифест) работают. Заложена архитектура микрофронтендов, позволяющая независимо разрабатывать и деплоить разные разделы платформы.

## Этап 0.7: Конфигурация деплоя фронтенда и безопасности.

•	Цель: Обеспечить правильную сборку и доставку фронтенд-модулей через CDN, настроить базовые политики безопасности.
•	Задействованные компоненты: Netlify (хостинг статики), Cloudflare CDN и настройки заголовков.
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-фронтенд-разработчик.
•	Задачи:
o	Настроить сборку каждого фронтенд-модуля для продакшена: добавить конфигурацию netlify.toml или аналоги в проекты модулей, чтобы Netlify мог автоматически билдить и развернуть их. Рассмотреть схему деплоя, при которой App Shell и модули деплоятся раздельно: оболочка как основное приложение, а микрофронтенды – как статические файлы на CDN.
o	Подключить глобальный CDN: убедиться, что статика (JS/CSS, изображения) раздаётся через Cloudflare для минимальной задержки по всему миру. Провести тестовые развертывания: после пуша в main Netlify публикует приложение, Cloudflare кеширует контент.
o	Включить основные заголовки безопасности HTTP: Content Security Policy (CSP) для ограничения источников, HSTS для HTTPS, и т.д. Настроить их через Cloudflare либо Netlify конфигурацию.
•	Результат: Фронтенд-сборки успешно деплоятся в облако, все страницы доступны через HTTPS. Статические файлы обслуживаются CDN Cloudflare. Базовые меры безопасности на месте. По завершении фазы 0 весь каркас инфраструктуры готов к развитию функционала: разработчики имеют настроенное окружение, и можно приступать к созданию пользовательских модулей.

---

# Фаза 1: Pilot Modules (контентные модули и базовые сервисы)

Цель фазы 1: Быстро разработать и запустить основные контентные разделы платформы (Atlas, Pulse, Blog) и необходимые базовые микросервисы для старта экосистемы. Итогом фазы станет MVP-платформа с контентным наполнением и базовой механикой поинтов, доступная первым пользователям. Фронтенд-разработка в виде PWA-модулей, бэкенд –микросервисы.

## Этап 1.1: Реализация Atlas Asia (контентный модуль локаций).

•	Цель: Создать раздел Atlas Asia – энциклопедию мест Юго-Восточной Азии, с возможностью офлайн-доступа к справочной информации.
•	Задействованные компоненты: Фронтенд-модуль Atlas; данные локаций (временно во внешнем CMS/JSON); Content API (чтение).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Content), ИИ-аналитик контента.
•	Задачи:
o	Разработать интерфейс Atlas: страницы стран, городов, отдельных мест (достопримечательностей, заведений). Предусмотреть иерархию “страна → город → место” с навигацией.
o	Обеспечить кэширование контента для офлайна: последние просмотренные страницы мест сохраняются локально, чтобы пользователь мог видеть информацию без подключения. Добавить уведомление о режимe офлайн.
o	Интегрироваться с Content API для получения данных: при открытии страницы места фронтенд запрашивает описание, фото, координаты из бэкенда (пока данный сервис эмулируется заглушкой или простым JSON).
•	Результат: Модуль Atlas Asia функционирует: пользователь может просматривать справочные статьи о локациях, изучать описания. На этапе MVP данные могут быть частично захардкожены или импортированы из внешнего headless CMS, но единая точка доступа – через Content API – уже определена. Atlas закладывает фундамент геоданных для всей экосистемы.

## Этап 1.2: Реализация Pulse Asia (лента событий).

•	Цель: Запустить раздел Pulse Asia – календарь-афишу актуальных событий региона (мероприятия, встречи, события сообществ).
•	Задействованные компоненты: Фронтенд-модуль Pulse; данные событий; Content API (чтение).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Content).
•	Задачи:
o	Создать интерфейс календаря (месяц, неделя, день) и ленты событий: карточки событий с названием, датой/временем, категорией и местом проведения. Реализовать фильтрацию и поиск по тегам, дате, локации, чтобы пользователи могли находить интересующие события.
o	Обеспечить обновление контента через API: фронтенд периодически опрашивает бэкенд за новыми событиями или получает их при перезагрузке. Для офлайн-режима – кешировать несколько последних или ближайших по времени событий, позволяя просматривать их детали без сети.
o	Интегрировать с геоданными Atlas: каждое событие привязано к месту проведения. На этапе MVP – хранить ID места (из Atlas) или текстовое название; при отображении событий показывать привязанный город/локацию. В будущем Pulse будет использовать Atlas-сервис напрямую по идентификатору места.
•	Результат: Пользователи могут открывать ленту событий Pulse Asia и просматривать, что интересного происходит вокруг. Интерфейс поддерживает базовые фильтры, позволяет увидеть детали события. При отсутствии интернета видна информация о некоторых сохранённых ранее событиях. Этот модуль привлекает аудиторию актуальностью контента и станет основой для социального взаимодействия (в дальнейшем события будут интегрироваться с соцсетью Space и партнерскими программами).

## Этап 1.3: Реализация Blog Asia (медиаплатформа).

•	Цель: Запустить Blog Asia – раздел статей, новостей и историй, формирующих медиасоставляющую экосистемы.
•	Задействованные компоненты: Фронтенд-модуль Blog; контент статей; внешняя CMS/Markdown для ввода; Content API (чтение).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Content), ИИ-аналитик контента.
•	Задачи:
o	Создать интерфейс блога: лента статей с категоризациями (рубрики, теги), страницы чтения статьи с заголовком, текстом, галереей изображений. Внедрить SSR/SSG (серверный рендеринг или статическую генерацию) для страниц статей ради SEO, чтобы поисковики индексировали контент.
o	Реализовать базовый механизм комментариев для статей: на MVP-компромисс – встроить сторонний виджет (например, Disqus/Giscus), чтобы пользователи могли обсуждать статьи. Собственная система комментариев отложена на пост-MVP.
o	Подключить контентную систему: на первом этапе редакция может наполнять блог через внешние инструменты (например, Markdown-репозиторий на GitHub). Настроить периодическую выгрузку или импорт этих материалов в Content API, либо прямое обращение фронтенда к статически сгенерированным страницам.
•	Результат: Модуль Blog Asia отображает список опубликованных статей и позволяет читать их со всеми медиавложениями. Статические страницы генерации обеспечивают индексируемость в поисковых системах (важно для привлечения органического трафика). Пользователи могут оставлять комментарии (посредством внешнего виджета). Контентная команда получает канал для доставки новостей и историй, что увеличивает ценность платформы для аудитории.

## Этап 1.4: Интеграция контентных модулей в PWA-оболочку.

•	Цель: Объединить Atlas, Pulse, Blog в едином фронтенде, настроить их динамическую загрузку и навигацию между ними.
•	Задействованные компоненты: App Shell (фронтенд оболочка); модули Atlas/Pulse/Blog; механизм ленивой загрузки.
•	Вовлечённые агенты: ИИ-фронтенд-разработчик.
•	Задачи:
o	Добавить в App Shell пункты меню/ссылки для новых разделов: “Atlas”, “Pulse”, “Blog”. Привязать их к роутерам микрофронтендов.
o	Настроить Module Federation (или аналог) для загрузки сборок: при первом заходе в раздел Atlas Shell загружает удалённый бандл Atlas Asia, аналогично для Pulse и Blog. Убедиться, что бандлы раскладываются по кэшу и не влияют на начальную загрузку приложения (пользователь сначала видит только оболочку).
o	Реализовать бесшовную навигацию: например, при клике на место в Guru (будет реализован позже) открывается страница этого места в Atlas; пока, внутри этих трёх модулей обеспечить переходы (из статьи блога, упоминающей место, – ссылка на Atlas; из описания события – ссылка на место Atlas, и т.д.). Заложить единый стиль карточек и ссылок между модулями.
•	Результат: Три пилотных модуля интегрированы в единое приложение. Пользователь может переключаться между ними через общее меню; при этом благодаря ленивой загрузке первоначальная загрузка остаётся лёгкой, а новые разделы подгружаются по мере необходимости. Это подтверждает работоспособность архитектуры микрофронтендов и демонстрирует цельную платформу даже при отдельных модулях.

## Этап 1.5: Разработка Auth Service (базовый сервис аутентификации).

•	Цель: Запустить собственный бэкенд-сервис аутентификации для управления пользователями и интеграции с SSO.
•	Задействованные компоненты: Auth Service (микросервис на Cursor); интеграция с Clerk (вебхуки); БД профилей пользователей.
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (Auth), ИИ-DevOps-инженер.
•	Задачи:
o	Реализовать приём вебхуков от Clerk: при событии создания нового пользователя Clerk отправляет данные в Auth Service. Сервис сохраняет дополнительные поля профиля в своей базе – например, реферальный код пользователя, роль (VIP/PRO) или настройки, не покрытые Clerk.
o	Организовать выдачу токенов сессии: хотя Clerk сам обеспечивает JWT, продумать эндпоинты Auth Service, через которые фронтенды могут запросить актуальный JWT и базовую информацию о пользователе.
o	Подготовить задел для будущих расширений: оставить в коде возможности для альтернативных методов входа (например, миграция от Clerk к своей базе) и для админ-функций (блокировка пользователя, обновление ролей и пр.). Пока эти функции не реализуются, но архитектура сервисa должна позволять их добавление.
•	Результат: Auth Service запущен и подключён к экосистеме. Он выступает прослойкой между внешним SSO и внутренними сервисами: хранит внутренние пользовательские данные и в дальнейшем сможет брать на себя часть логики аутентификации. На этапе MVP его функции ограничены проксированием Clerk и сохранением доп. сведений, но единая точка контроля доступа определена.

## Этап 1.6: Разработка Content API Service (единая точка доступа к контенту).

•	Цель: Обеспечить фронтендам унифицированный Content API для всех контентных данных (статьи, локации, события и т.д.) на этапе MVP.
•	Задействованные компоненты: Content API Service (микросервис на Cursor); общая база данных контента (PostgreSQL); модели данных Post, Place, Event.
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (Content), ИИ-архитектор БД.
•	Задачи:
o	Спроектировать схемы данных для основных сущностей: Place (для Atlas: название, координаты, описание, теги), Event (для Pulse: название, время, привязка к Place), Post (для Blog: заголовок, текст, теги, автор) и связанные справочники (например, Tag). Реализовать миграции, создать таблицы.
o	Реализовать REST/GraphQL API для чтения этих данных: например, GET /api/content/places?country=VN возвращает список мест во Вьетнаме, GET /api/content/posts?tag=travel – статьи с тегом travel. Добавить базовые фильтры и пагинацию на уровне API.
o	Объединить источники данных Atlas/Pulse/Blog: на фазе 1 Content Service может работать как промежуточный слой. Частично контент (например, текст статей) может по-прежнему жить во внешнем CMS и импортироваться скриптами, но фронтенды всегда обращаются только к Content API – так достигается консистентность и единая точка чтения уже на MVP.
•	Результат: Функционирует единый контентный бэкенд. Atlas, Pulse, Blog получают данные не напрямую из разрозненных источников, а через Content API, что упрощает фронтенд-код и обеспечивает единообразие. Хотя под капотом сервис пока монолитно обслуживает разные типы контента, в будущем (фазы 2–3) планируется выделение отдельных специализированных сервисов (например, отдельный сервис геолокаций Atlas). MVP-реализация минимальна, но создаёт фундамент для наращивания контентной платформы.

## Этап 1.7: Разработка Referral Service (реферальная программа).

•	Цель: Внедрить механизм реферальных ссылок и бонусов, стимулирующий рост пользовательской базы с самого запуска.
•	Задействованные компоненты: Referral Service (микросервис на Cursor); связка с Auth/Clerk (вебхуки регистраций); база рефералов.
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (Referral), ИИ-архитектор бизнес-логики.
•	Задачи:
o	Реализовать генерацию реферальных кодов: при регистрации нового пользователя создаётся уникальный код (например, короткий алфавитно-цифровой). Предусмотреть эндпоинт для получения своего кода и регистрации приходящих по нему (POST при пригласительном линке).
o	Отслеживать “кто кого пригласил”: хранить в БД связь inviter → invited_user. При регистрации по ссылке, Clerk через вебхук сообщает Referral Service о новом пользователе с указанным кодом. Сервис сохраняет связь и помечает, что пригласитель заслужил награду.
o	Начислять базовые бонусы: при срабатывании рефералки выдать пригласившему пользователю поинты (через Token Service, см. следующий этап). Логика на MVP простая: единоразовый бонус фиксированной величины, без уровней. В будущем (фаза 2) будет расширен кабинет Connect с реферальной статистикой и многоуровневой системой, но сейчас важно заложить основу.
•	Результат: Сервис рефералов начинает работать в фоне платформы. Пользователи сразу при запуске Blog/Pulse могут приглашать друзей и получать за это вознаграждения. Несмотря на отсутствие отдельного фронтенда для отображения реферальной статистики на MVP (он появится в фазе 2 как модуль Connect Asia), данные собираются с первого дня. Это поможет быстро нарастить аудиторию: реферальный механизм будет одной из первых вовлекающих метрик платформы.

## Этап 1.8: Разработка Token Service (внутренняя валюта Points).

•	Цель: Ввести внутреннюю нефинансовую валюту лояльности Points и базовые правила её начисления для геймификации.
•	Задействованные компоненты: Token Service (микросервис на Cursor); база балансов Points; API начисления/списания.
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (Token), ИИ-архитектор бизнес-логики.
•	Задачи:
o	Создать модель хранения баланса Points: таблица пользователей с текущим количеством поинтов и таблица транзакций (начисление, списание) для истории.
o	Определить события для начисления поинтов на MVP: регистрация по реферал-коду (основной случай), а также активности пользователя – например, созданный пост в соцсети, оставленный комментарий или лайк. За каждое событие – фиксированное небольшое число Points.
o	Реализовать API сервиса: метод получения своего баланса (для будущего кабинета), метод начисления Points внешним сервисом. Например, Referral Service будет вызывать POST /api/token/add с указанием пользователя и суммы при срабатывании рефералки. Сервис обновит баланс и запишет транзакцию.
•	Результат: Token Service функционирует, начисляя очки лояльности за простейшие достижения. Points пока никак не конвертируются в реальные ценности (это произойдёт на фазе 3 с вводом крипто-токена), они используются лишь для геймификации (рейтингов, поощрения активности). Тем не менее, уже на MVP пользователи видят накопленные поинты, что поощряет их приглашать друзей и участвовать в жизни платформы. Логика сервиса предельно проста и безопасна (никаких операций с реальными деньгами), что позволило быстро её реализовать.

## Этап 1.9: Интеграция, тестирование и запуск MVP.

•	Цель: Собрать вместе все созданные в фазе 1 компоненты, проверить их совместную работу и развернуть платформу для первых пользователей.
•	Задействованные компоненты: Фронтенды Atlas/Pulse/Blog (собраны), микросервисы Auth/Content/Referral/Token, CI/CD pipeline.
•	Вовлечённые агенты: ИИ-менеджер проекта (оркестратор), ИИ-DevOps-инженер, ИИ-тестировщик, ИИ-фронтенд и бэкенд-разработчики.
•	Задачи:
o	CI/CD деплой: убедиться, что при слиянии кода всех модулей GitHub Actions выполняет сборку фронтендов (npm build) и автоматически загружает их на Netlify. Настроить деплой микросервисов: через Docker push и запуск на сервере или в контейнерной платформе (например, Fly.io). Проверить, что все API-сервисы доступны по правильным URL (например, api.go2asia.space/content, .../auth и т.д.) и защищены Cloudflare WAF.
o	Сквозное тестирование: QA-агент проводит end-to-end тесты ключевых сценариев: новая регистрация (через фронт Blog, например) -> получение JWT -> загрузка контента (статей, мест, событий) -> начисление реферального бонуса. Проверяется, что при регистрации по приглашению рефералка срабатывает (Points начислены). Также тестируется вход под разными ролями (обычный, PRO – пока разница минимальна), офлайн-функциональность модулей.
o	Полный запуск MVP: переключить публичный домен на собранное приложение, открыть доступ ограниченной аудитории. Организовать мониторинг ошибок (подключить Sentry или вывести логи), чтобы сразу реагировать на сбои.
•	Результат: Платформа Go2Asia запущена в базовом виде. Пользователи могут регистрироваться через Clerk, после чего им доступен контент: они просматривают статьи, события, места. За приглашение друзей начисляются поинты. Offline-режим, кэширование, SSO – все основные механики проверены на практике. Этап MVP завершён: проект переходит от прототипа к работающему продукту, готовому к накоплению контента и пользователей. Социальные функции, партнерские сервисы и крипто-экономика пока не реализованы – они планируются на последующие фазы (post-MVP).

---

# Фаза 2: Core Modules (социальные и партнерские функции, расширение бэкенда)

Цель фазы 2: Расширить платформу ключевыми интерактивными сервисами (соцсеть, геймификация, партнерские программы) и усилить бэкенд для поддержки роста аудитории. В этой фазе появляются новые фронтенд-модули с богатыми пользовательскими возможностями и дополнительные микросервисы. Также внедряются общие инфраструктурные компоненты: хранение пользовательских файлов и полнотекстовый поиск. Благодаря мультиагентному подходу, команда может параллельно разрабатывать несколько модулей: пока один агент-кодер создает соцсеть Space, другой – модуль Quest, третий – партнерский каталог RF, что значительно ускоряет поставку функционала. Ниже разбивка на этапы фазы 2.

## Этап 2.1: Запуск Space Asia (соцсеть) и бэкенд Space Service.

•	Цель: Ввести полноценную социальную сеть Space Asia, чтобы пользователи могли общаться, делиться контентом и формировать сообщество внутри платформы.
•	Задействованные компоненты: Фронтенд-модуль Space; Space Service (микросервис соцсети); уведомления (push/чат).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Space), ИИ-аналитик UX, ИИ-тестировщик.
•	Задачи:
o	Разработать фронтенд Space: реализовать ленту постов (feed) с бесконечной прокруткой, экран профиля пользователя (со списком его постов, друзей/подписчиков), базовый функционал групп или сообществ. Сделать упор на удобство с мобильных устройств: мгновенная подгрузка ранее просмотренных постов (кеш фида), возможность писать посты офлайн (сохранять черновик и публиковать при появлении связи).
o	Интегрировать push-уведомления: подключить на фронте SDK OneSignal или Firebase Messaging. Настроить в сервис-воркере App Shell подписку на уведомления и обработку событий (новый лайк, комментарий, ответ в чате).
o	Разработать Space Service (бэкенд): создать модели Post (пост пользователя с текстом, медиа, приватностью), Comment, Friend/Follow (отношения между пользователями), Notification. Реализовать API: создание поста, получение ленты (сортировка по времени и подпискам), добавление в друзья/подписки, список уведомлений. Для обновления фида в режиме реального времени можно запланировать WebSocket или SSE, но на этом этапе можно обойтись периодическими запросами.
•	Результат: Социальный модуль Space Asia интегрирован в платформу (в App Shell добавлена вкладка “Space/Профиль”). Пользователи могут создавать записи, комментировать, подписываться друг на друга. Бэкенд обрабатывает хранение и выдачу этих данных. Push-уведомления настроены: при подключении к интернету пользователи получают оповещения о новой активности. Сообщество начинает формироваться, повышая вовлечённость аудитории.

## Этап 2.2: Запуск Connect Asia (реферально-партнёрский кабинет).

•	Цель: Предоставить пользователям (и особенно PRO/партнёрам) кабинет Connect Asia для отслеживания реферальной программы, баланса поинтов и партнерской активности.
•	Задействованные компоненты: Фронтенд-модуль Connect; расширенный функционал Referral Service и Token Service; (будущий RF Partner Service).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Referral/Token), ИИ-аналитик бизнес-метрик.
•	Задачи:
o	Разработать фронтенд Connect: страницы “Мой кошелёк” с отображением текущих Points и (в будущем) токенов, список приглашённых друзей (реферальное дерево 1-2 уровня), блок достижений (статус VIP, заработанные награды). Для PRO-кураторов и бизнес-партнёров – отдельные разделы: статистика привлечённых пользователей, бонусы за их активность, заявки новых партнёров (некоторые из этих функций реализуются позже, но UI можно предусмотреть).
o	Интегрировать данные из сервисов: подключить Referral Service и Token Service к фронтенду Connect. Например, для отображения дерева рефералов – вызывать API Referral, для списка транзакций и баланса Points – API Token Service. Добавить фильтры по дате, типу транзакций (начисление/списание).
o	Расширить бэкенд для партнеров: на фазе 2 логика работы с бизнес-партнёрами может быть частично добавлена в Referral Service (или вынесена, при необходимости, в отдельный RF Partner Service). Например, реализовать учёт подключённых RF-партнёров: PRO-кураторы могут через Connect подать заявку на добавление нового партнёра; такие заявки сохраняются и помечаются для дальнейшей обработки (пока без автоматизации). Также подготовить API для получения списка партнёров, привязанных к PRO (для начисления комиссий – детали в фазе 3).
•	Результат: В платформе появляется раздел Connect (доступный из меню, а также по URL /connect). Обычные пользователи видят там свой прогресс: сколько рефералов пригласили, сколько Points заработали, какие статусы достигли. PRO-кураторы начинают использовать его для отслеживания подключённых ими бизнесов и базовой статистики. Таким образом, реферальная экономика становится прозрачнее для пользователя, повышая мотивацию участвовать в росте платформы. (Полноценная токеномика и расчёты вознаграждений реализуются в фазе 3.)

## Этап 2.3: Запуск Russian Friendly (каталог партнёров) – публичная витрина.

•	Цель: Добавить модуль Russian Friendly (RF), представляющий каталог бизнес-партнёров, каталог товаров/услуг и ваучеров, предлагающих особые условия для русскоговорящих путешественников.
•	Задействованные компоненты: Фронтенд-модуль RF; данные партнёров (названия, описание, скидки); (бэкенд: частично Referral/Partner Service).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Referral/Partner), ИИ-аналитик партнёрств.
•	Задачи:
o	Реализовать интерфейс каталога RF: список партнёров (отели, рестораны, сервисы) с фильтрами по категории, локации, наличию скидок. Страница партнёра с описанием, отметкой RF (значок подтверждённого Russian Friendly), условиями для туристов (например, “скидка 10% по коду…”). Добавить поиск по названию.
o	Обеспечить офлайн-доступ к каталогу: при первом открытии загрузить и сохранить список основных партнёров (или тех, что пользователь просматривал), чтобы в поездке без интернета можно было открыть последний список.
o	Настроить получение данных: на этом этапе можно хранить список партнёров в базе Referral Service (или отдельной таблице Partner). Фронтенд RF запрашивает, например, GET /api/referral/partners?city=... чтобы получить список. Также подготовить в Referral API возможность зарегистрировать нового партнёра (пока без UI, а через БД или админку, см. этап про админ-панель).
•	Результат: На платформе появляется раздел Russian Friendly (/rf). Он представляет ценность для туристов, показывая, где их ждёт дружелюбный сервис. Пока функциональность ограничена чтением каталога, но уже создаётся точка притяжения и для бизнеса: партнёры видят, что их компания может быть представлена аудитории. Кабинет партнёра для управления своим профилем не реализован на фазе 2, однако в фазе 3–4 будет добавлен. RF-модуль тесно интегрируется с Atlas (использует его места) и Guru (отображает партнёров на карте рядом).

## Этап 2.4: Запуск Quest Asia (система квестов) и бэкенд Quest Service.

•	Цель: Внедрить геймификацию через модуль Quest Asia – пользователи выполняют задания и получают вознаграждения, что повышает вовлечённость.
•	Задействованные компоненты: Фронтенд-модуль Quest; Quest Service (микросервис на Cursor); интеграция с Token Service (награды).
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Quest), ИИ-аналитик геймификации, ИИ-тестировщик.
•	Задачи:
o	Создать интерфейс Quest: список доступных квестов (миссий) с описанием условий (напр., “Посети место X”, “Пройди маршрут Y”, “Напиши пост о поездке”), страницу конкретного квеста с прогрессом. Реализовать отслеживание шагов: отмечать чек-пойнты (например, кнопка “Отметиться на месте” – при нахождении в координатах места).
o	Учесть офлайн: активные квесты сохранять локально, чтобы пользователь мог отмечать выполнение без сети. Сделать синхронизацию при появлении интернета (например, с помощью Background Sync API).
o	Разработать Quest Service: модель Quest (описание, условия, награда Points/NFT), QuestProgress (состояние выполнения по пользователям). Реализовать API: старт квеста (помечает пользователя как участника), отметка шага выполненным, завершение квеста. При завершении – выдать награду: вызвать Token Service для начисления Points, а также зафиксировать получение NFT-бэйджа (пока просто запись в прогрессе, сами NFT будут выпущены позже). Предусмотреть связь с Space (например, пост-отчёт в соцсети о выполненном квесте).
•	Результат: Квестовая механика запускается. Пользователи могут участвовать в игровых активностях, что оживляет платформу: за реальную исследовательскую деятельность (посещение мест, участие в событиях) они получают награды. После выполнения квеста Points автоматически начисляются (Token Service вызывается Quest-сервисом), данные о достижении могут отправляться в ленту (Space) или профиль. Этот модуль стимулирует изучение контента платформы (например, через задания “прочти статью о культуре Вьетнама”) и расширяет взаимодействие.

## Этап 2.5: Запуск Rielt.Market Asia (аренда жилья) и бэкенд Rielt Service.

•	Цель: Добавить модуль Rielt.Market Asia – поиск и размещение объявлений о долгосрочной и краткосрочной  аренде жилья, интегрированный с экосистемой.
•	Задействованные компоненты: Фронтенд-модуль Rielt; Rielt Service (микросервис объявлений); интеграция с Atlas и RF.
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Rielt), ИИ-аналитик продуктовых метрик, ИИ-тестировщик.
•	Задачи:
o	Реализовать интерфейс Rielt.Market: каталог объявлений жилья с фильтрами (город, цена, тип жилья, наличие RF-статуса). Карточка жилья с описанием, фото, ценой, пометкой RF, контактами или кнопкой связи. Предусмотреть две роли: арендатор (поиск жилья) и владелец (публикация объявления). Пока на фазе 2 акцент на поиск, интерфейс подачи объявлений может быть упрощённым или доступен только PRO-пользователям.
o	Разработать Rielt Service: хранение объявлений (модель Listing: описание, цена, координаты, флаг RF, владелец), CRUD API для объявлений (создать/просмотреть). При добавлении нового объявления – геокодировать адрес через Atlas: получить ID города/района и координаты места, не дублируя текст адреса. Если владелец – партнёр RF, помечать объявление соответствующим значком.
o	Интеграция с другими сервисами: привязать Rielt к Atlas (использование справочника городов/мест при публикации, чтобы не вводить несуществующие локации). Обеспечить, чтобы Guru Service мог получать список объявлений рядом (эндпоинт /listings/nearby по координатам) и включать жильё в общий вывод “рядом с тобой”. Также учесть участие PRO-кураторов: можно предусмотреть, что PRO могут проверять и верифицировать объекты жилья, повышая доверие (эти механики будут рассчитаны в токеномике фазы 3).
•	Результат: Модуль Rielt.Market запущен и позволяет искать жильё для аренды. Пользователь, открыв раздел, видит доступные квартиры/дома, может применить фильтры. При клике на объявление – подробная информация, отметка RF (если применимо). Данные объявлений хранятся в Rielt Service и интегрируются с остальными: например, в Guru на карте отображаются объекты жилья рядом наравне с местами и событиями. Таким образом, Go2Asia начинает покрывать ключевую потребность путешественников – поиск жилья, дополняя информационные сервисы практическим инструментом.

## Этап 2.6: Запуск Guru Asia (гео-агрегатор “рядом со мной”) и бэкенд Guru Service.

•	Цель: Реализовать модуль Guru Asia – главный экран для пользователя-путешественника, агрегирующий все объекты вокруг него (достопримечательности, события, жильё, люди и пр.) на карте.
•	Задействованные компоненты: Фронтенд-модуль Guru; Guru Service (микросервис-агрегатор); интеграция со всеми контентными сервисами.
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Guru), ИИ-аналитик UX, ИИ-тестировщик.
•	Задачи:
o	Разработать фронтенд Guru: карту (на базе Leaflet или Mapbox GL) и сопутствующий список объектов. Использовать геолокацию пользователя (с разрешения) для определения центра карты. Отображать пины/маркеры и список объектов поблизости: места (из Atlas), события (Pulse), жильё (Rielt), активные PRO (Space, напр. отмеченные геопозиции), активных партнёров (RF), квесты (точки Quest). Обеспечить единый стиль карточек в списке, с указанием типа объекта и расстояния.
o	Оффлайн-режим: реализовать агрессивное кеширование: сохранять результаты последнего запроса (например, 5-10 объектов вокруг последней известной локации) в IndexedDB. Если сеть недоступна, Guru показывает последние сохранённые места/события, а не пустой экран.
o	Разработать Guru Service: этот бэкенд работает по принципу Backend-for-Frontend, объединяя данные из разных сервисов. Реализовать эндпоинт, например, GET /api/guru/nearby?lat=X&lng=Y, который внутри делает параллельные запросы в Atlas (места), Pulse (события), Rielt (жильё), Space (активные пользователи?) и собирает их результаты. Сервис нормализует полученные данные в единый формат (например, унифицированная структура EntityCard с полями: тип, название, описание, координаты, ссылка на исходный модуль). В ответ клиенту возвращается список объектов вокруг. Добавить кэширование запросов по тайлам (область карты) на небольшой промежуток, чтобы при повторных запросах не дергать все сервисы. Также реализовать устойчивость: если какой-то сервис не ответил, Guru возвращает данные от остальных (частичный результат).
•	Результат: Guru Asia становится центральной точкой входа для пользователя-туриста. Открыв приложение, он сразу видит на карте и в списке, что находится рядом – достопримечательности, мероприятия, жильё и активных участников сообщества (PRO-спейсеров). Из Guru удобно провалиться глубже: кликнув на объект, пользователь переходит в соответствующий модуль (карточка места ведёт в Atlas, карточка события – в Pulse, и т.д.). Бэкенд Guru Service выполняет роль “клея”, скрывая сложность множества API за единым простым вызовом. Этот модуль существенно улучшает UX, предлагая единое окно взаимодействия с разными частями платформы.

## Этап 2.7: Расширение инфраструктуры хранения и поиска.

•	Цель: Внедрить общие сервисы хранения файлов и полнотекстового поиска, необходимые для новых функций.
•	Задействованные компоненты: Облачное хранилище (Cloudflare R2); поисковый движок (например, Meilisearch или ElasticSearch).
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-бэкенд-разработчик (поиск), ИИ-архитектор.
•	Задачи:
o	Централизовать хранение медиа: настроить для всех модулей использование Cloudflare R2 (если не сделано ранее). Например, при загрузке аватара в Space или фото места в Atlas, файл отправляется в R2, а сервис сохраняет только ссылку. Гарантировать, что все микросервисы используют единый SDK или API для работы с хранилищем, чтобы избежать дублирования.
o	Поднять сервис поиска: развернуть контейнер Meilisearch (легкий и быстрый поисковый движок) либо ElasticSearch. Проиндексировать ключевые данные: места Atlas, события Pulse, статьи Blog, посты Space. Настроить обновление индекса при добавлении нового контента (например, по событию или cron).
o	Добавить глобальный поиск в фронтенд: в App Shell (шапке) реализовать строку поиска, отправляющую запрос в отдельный API (Search Service или непосредственного движка) и возвращающую смешанные результаты – подсказки по городам, людям, статьям и т.п. Это потребует унификации хранения индексов (как вариант, поддерживать индекс каждой сущности отдельно и агрегировать результаты на фронте).
•	Результат: Платформа получает поисковую функциональность: пользователи могут искать по ключевым словам и находить релевантный контент во всех разделах. Производительность поиска высокая, т.к. специализированный движок оптимизирован под автодополнение и ранжирование. Также теперь любые пользовательские загрузки (фото, видео) хранятся в надёжном и масштабируемом облаке, что важно для дальнейшего роста объёма данных.

## Этап 2.8: Разработка Admin Panel (административная панель).

•	Цель: Создать единый интерфейс модерации и управления контентом для редакторов и администраторов экосистемы Go2Asia.
•	Задействованные компоненты: Фронтенд-модуль Admin; интеграция с API Atlas/Pulse/Blog/Space/Quest/RF; роль Admin/Editor в SSO.
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (интеграция API), ИИ-аналитик (оптимизация контент-процессов).
•	Задачи:
o	Реализовать PWA-приложение Admin Panel, доступное по отдельному пути (например, /admin) и только для пользователей с ролью Admin или Editor (Clerk SSO должен предоставлять эту роль). Интерфейс построить модульно: разделы “Atlas” (управление справочником мест), “Pulse” (модерация событий), “Blog” (редактирование статей), “Space” (модерация постов/комментариев), “Quest” (управление квестами), “RF” (модерация партнёров), “Connect” (просмотр рефералов/транзакций).
o	Каждому модулю – свой CRUD-интерфейс: например, в Atlas – таблица стран/городов/мест, возможность добавить/редактировать место, загрузить фото. В Space – список жалобных постов или недавно созданных, с возможностью скрыть/забанить; в Blog – редактор статьи (текст, изображения, теги) и управление статусом (черновик/публикация/отклонение).
o	Интегрировать Admin Panel с существующими API микросервисов: никаких прямых подключений к базам – всё через HTTP-запросы к Atlas Service, Content API, Space Service и пр. Таким образом, админка действует как внутренний клиент к тем же сервисам, которыми пользуются фронтенды, только с расширенными правами. Обеспечить, чтобы при редактировании данных в админке они сразу отражались на пользовательской стороне (за счёт вызова тех же API – данные обновляются в реальном времени).
•	Результат: Внутренняя команда Go2Asia получает удобный инструмент для поддержки экосистемы. Отпадает необходимость заполнять БД скриптами или пользоваться внешними CMS – весь контент теперь управляется через единый веб-интерфейс. Модераторы могут оперативно реагировать на неприемлемый пользовательский контент (например, удалить оскорбительный пост), редакторы – публиковать новые статьи и события. Это повышает качество и актуальность контента к моменту финального релиза.

## Этап 2.9: Внедрение шины событий (Event Bus).

•	Цель: Организовать асинхронное взаимодействие между микросервисами для лучшей масштабируемости и расширяемости.
•	Задействованные компоненты: Система очередей/шины (например, Cloudflare Queues или аналог); интеграция микросервисов через события.
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (интеграция), ИИ-DevOps-инженер.
•	Задачи:
o	Развернуть инфраструктуру очередей: выбрать решение (например, Nats, RabbitMQ, или облачную шину Valhalla Streams). Интегрировать его в проект (добавить конфигурацию в infra/, запустить сервис).
o	Определить формат событий и ключевые топики: например, событие quest.completed (пользователь завершил квест), post.created (новый пост в Space), partner.joined (новый партнер зарегистрирован). Задать схему сообщений (JSON с необходимыми полями).
o	Доработать микросервисы для публикации/подписки: Quest Service при завершении квеста шлёт событие quest.completed с указанием userId и questId; Referral Service подписывается на quest.completed и если участник пришёл по его реферал-ссылке – начисляет бонус; Notification Service (если будет) подписывается на quest.completed чтобы отправить поздравление; Space Service подписывается на blog.article.published (если статья опубликована – создать пост-анонс). Имплементировать хотя бы несколько таких связок, критичных для текущей логики.
•	Результат: Микросервисы начинают общаться не только напрямую (через REST), но и через асинхронные события. Это снижает их связанность – сервисы реагируют на события независимо, что повышает устойчивость системы: сбой или задержка в одном не блокирует других. Кроме того, это упрощает добавление новых функций – достаточно “вклинить” новый сервис, подписавшийся на нужное событие. Подготовленная шина событий заложит основу для более сложных механик (например, аналитика или динамическая настройка контента) на следующих фазах.

## Этап 2.10: Тестирование интеграции и развертывание обновлений (конец фазы 2).

•	Цель: Убедиться в корректной работе всех новых модулей и сервисов вместе с компонентами MVP, провести нагрузочные тесты ключевых функций и выпустить обновление платформы.
•	Задействованные компоненты: Новые фронтенды (Space, Connect, RF, Quest, Rielt, Guru, Admin); новые сервисы (Space, Quest, Guru, расширенные Referral/Token); CI/CD.
•	Вовлечённые агенты: ИИ-менеджер проекта, ИИ-тестировщик, ИИ-DevOps-инженер, профильные разработчики.
•	Задачи:
o	Параллельное тестирование модулей: QA-агент проводит сценарные тесты: создание поста в Space -> появление его в фиде подписчика; выполнение квеста -> начисление Points; добавление партнёра через админку -> отображение в каталоге RF; создание объявления жилья -> появление его в Guru рядом. Особое внимание – стыкам между сервисами (например, корректно ли Guru отображает данные при недоступности одного из источников).
o	Производительность: Прогнать нагрузочные тесты на соцсеть Space (имитация N тысяч постов и M пользователей в ленте) и Guru (частые гео-запросы). Выявить узкие места (например, отсутствие индексов в БД, долгое выполнение запросов Atlas при большом числе мест). Отразить эти проблемы в задачах следующей фазы (оптимизация).
o	CI/CD деплой: Выполнить автоматический деплой новой версии: убедиться, что все front-модули обновлены (в навигацию добавлены кнопки новых разделов, App Shell включает новые роуты), бэкенды запущены и их эндпоинты доступны. Организовать постепенное выкатывание: например, сначала развернуть сервисы, проверить их работу из админ-панели, затем включить фронтенды для всех пользователей.
•	Результат: Версия платформы после фазы 2 успешно развёрнута. Go2Asia теперь выходит за рамки контентного портала, превращаясь в социально-ориентированный продукт: пользователи могут взаимодействовать друг с другом, выполнять задания, пользоваться полезными сервисами (жильё, каталог партнёров). База технических компонентов тоже выросла, но за счёт модульной архитектуры обновление прошло без нарушений работы MVP-функций. Система готова к переходу на следующий этап – внедрению собственной экономики и интеграции с блокчейном.

---

# Фаза 3: Интеграция и экономика (крипто-токены, токеномика, аналитика)

Цель фазы 3: Превратить экосистему Go2Asia в самодостаточную экономику и максимально связать разрозненные сервисы в единое пространство ценностей. На этом этапе внедряются блокчейн-компоненты (собственный крипто-токен G2A, NFT), механики децентрализации (DAO, голосования), а также системы аналитики и наблюдаемости. Пользователь получает единый кабинет, где собраны и социальные, и финансовые показатели, а команда – инструменты для глубокого анализа работы платформы.

## Этап 3.1: Внедрение Blockchain Wallet Gateway (шлюз для работы с блокчейном).

•	Цель: Реализовать сервис для интеграции с блокчейном TON, обеспечивающий двусторонний обмен между внутренней системой и внешними крипто-активами.
•	Задействованные компоненты: Wallet Gateway (микросервис или Cloudflare Worker); TON SDK; кошельки пользователей.
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (блокчейн-интеграция), ИИ-архитектор безопасности, (возможно ИИ-аналитик токеномики).
•	Задачи:
o	Разработать шлюз Off-chain ↔ On-chain: обеспечить возможность выводить внутриигровые ценности на блокчейн и заводить внешние токены в систему. Например, по запросу из Connect “выпустить NFT” – Gateway должен сгенерировать и отправить транзакцию в сеть TON для чеканки NFT-бейджа на адрес пользователя, затем зафиксировать в базе, что соответствующий внутриигровой бейдж стал on-chain NFT.
o	Интегрировать пользовательские кошельки: добавить в профиль пользователя возможность привязать TON Wallet (через TON Connect). Хранить публичный ключ/адрес. При действиях вроде покупки G2A токенов – принимать транзакции с этого адреса и сопоставлять с пользователем.
o	Обеспечить безопасность: приватные ключи для операций (например, чеканки NFT от имени системы) хранить в безопасном хранилище (HSM или зашифр. конфиг). Реализовать подтверждение транзакций – например, Gateway по webhook узнаёт о входящей транзакции, проверяет её в TON (с помощью SDK), и только затем начисляет эквивалентные ценности внутри системы.
•	Результат: В экосистеме появляется связь с внешним блокчейном. Пользователи могут выводить свои достижения наружу (NFT в кошельке) и приносить ценность извне (покупка G2A-токенов). Wallet Gateway становится мостом: он изолирует сложность работы с блокчейном от остальных сервисов, предоставляя им простой интерфейс (напр., “convertPointsToG2A(user, amount)”). Это открывает дорогу полноценной токеномике.

## Этап 3.2: Эволюция Token Service – поддержка крипто-токена G2A.

•	Цель: Расширить систему лояльности, введя реальный утилитарный токен G2A на блокчейне и обеспечив обмен между ним и существующими Points.
•	Задействованные компоненты: Модернизированный Token Service; интеграция с Wallet Gateway; смарт-контракт токена G2A (в сети TON).
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (Tokenomics), ИИ-архитектор, ИИ-специалист по безопасности.
•	Задачи:
o	Добавить учёт балансов G2A-токена: расширить модель Token Service, добавив счёт для крипто-токенов. Возможно, хранить не точное значение (которое на блокчейне), а кэш баланса, синхронизируемый через Gateway.
o	Реализовать конвертацию Points ↔ G2A: например, дать пользователю интерфейс обмена определённого количества Points на G2A-токены по курсу. При запросе обмена – Token Service списывает Points, вызывает Wallet Gateway для выпуска соответствующих G2A токенов на кошелёк пользователя. Обратный обмен – получить от Gateway событие о переводе X G2A на адрес платформы, после подтверждения – начислить пользователю Y Points.
o	Обновить механики начислений и трат: определиться, какие действия теперь требуют G2A. Например, PRO-кураторы могут платить G2A за продвижение своих событий или партнеры – за рекламу. Добавить в соответствующие сервисы обращения к Token Service для списания токенов. Обеспечить, чтобы эти транзакции также отражались в Connect (в истории операций).
•	Результат: В экосистеме появилась настоящая двухконтурная экономика. Points остаются внутрисистемной “очками” для геймификации, а G2A – полноценным цифровым активом. Пользователи могут обменивать одну валюту на другую, выводить токены наружу и наоборот. Это создаёт реальную ценность их активности: например, собрав много Points, можно конвертировать их в G2A и получить потенциал для покупок или инвестиций. С технической стороны, Token Service стал более сложным, но изолировал блокчейн-логику через Gateway, сохранив для внутренних модулей простой интерфейс.

## Этап 3.3: NFT-награды и ценность достижений.

•	Цель: Связать геймификацию с реальной ценностью через внедрение NFT-бейджей и коллекционных цифровых предметов.
•	Задействованные компоненты: Quest Service и Token Service (выдача NFT); Wallet Gateway (чеканка NFT); профиль пользователя (коллекция достижений).
•	Вовлечённые агенты: ИИ-бэкенд-разработчик (NFT), ИИ-фронтенд-разработчик (UI коллекций), ИИ-аналитик.
•	Задачи:
o	Чеканка NFT за достижения: определить, какие достижения будут подкрепляться NFT. Например, выполнение серии сложных квестов, достижение статуса TOP-10 контрибьюторов, или участие в запуске платформы. При наступлении события (Quest Service: quest.completed для особого квеста, Space: пользователь получил 1000 лайков и т.п.) – инициировать через Gateway выпуск NFT-бейджа на кошелёк пользователя[68]. NFT может содержать уникальную графику и метаданные (имя награды, дата).
o	Отображение NFT в профиле: обновить профиль (раздел Connect/Space) – добавить вкладку “Мои достижения” или “Коллекции”, где перечислены полученные NFT. Интегрировать с TON Explorer/API, чтобы можно было подтвердить владение. Если пользователь продаст NFT вне платформы, отобразить это (NFT может оставаться в списке, но с пометкой “выведен”).
o	Возможность приобретения/трейда: на этом этапе можно реализовать базовый обмен NFT между пользователями (но можно отложить). Главное – система должна позволять вывести награды в блокчейн, сделав их цифровыми сувенирами с потенциальной ценностью вне платформы.
•	Результат: Достижения пользователей выходят на новый уровень – помимо виртуальных очков и значков в профиле, они могут владеть реальными NFT. Это стимулирует активность: пользователи понимают, что, например, пройдя все квесты города, они получат редкий бейдж, которым можно похвастаться или даже продать на маркетплейсе. Для Go2Asia это формирует ранний рынок коллекционных ценностей вокруг платформы и повышает лояльность наиболее активных участников (элементы соревновательности, коллекционирования).

## Этап 3.4: Монетизация через статусы VIP и PRO

Цель: Ввести прямую монетизацию экосистемы Go2Asia без использования paywall или платного доступа к контенту.
Монетизация осуществляется исключительно через платные статусы VIP и PRO, которые расширяют возможности пользователей в реферальной экономике, Points/G2A-токеномике и экосистемных сервисах.
Задействованные компоненты: 
•	Connect Asia — управление статусами, интерфейсы покупки/продления.
•	Space Asia — отображение статусов, бейджи, визуальная идентификация.
•	Referral Service — расширенная матрица начисления Points для VIP/PRO.
•	Token Service — дополнительные права расходования Points для VIP/PRO.
•	RF Service / Quest Service — доступ к ваучерам и премиальным квестам.
•	Notification Service — уведомления о статусе и сроках окончания.
•	Платёжный шлюз (рублёвый эквайринг) — обработка VIP/PRO платежей.
Вовлечённые агенты:
•	ИИ-фронтенд-разработчик — интерфейсы покупки, отображение статусов, обновления в кабинете.
•	ИИ-бэкенд-разработчик — биллинг, статусы пользователей, реферальная логика.
•	ИИ-архитектор — моделирование статусов и их влияния на токеномику.
•	ИИ-аналитик — расчёт коэффициентов вознаграждений.
•	ИИ-специалист по безопасности — валидация платежей и статусов.
•	ИИ-технический писатель — документация по статусам и экономике.

Задачи:
1. Реализовать модель статусов и прав
•	Добавить статусы BASE, VIP, PRO в Auth/User Service.
•	Определить права и коэффициенты для VIP и PRO:
o	VIP: расширенные реферальные бонусы, право тратить Points на ваучеры/квесты.
o	PRO: углублённая реферальная сеть, возможность приводить RF-партнёров, доступ к созданию продвинутых событий/квестов.
2. Настроить оплату и биллинг статусов
•	Подключить рублёвый эквайринг.
•	Реализовать операции:
o	покупка VIP (1000 ₽/мес),
o	покупка PRO (30 000 ₽/год).
•	По успешной оплате:
o	Обновлять статус пользователя.
o	Записывать транзакцию в историю.
o	Уведомлять пользователя.
3. Интерфейсы покупки и управления статусами
•	Создать UI раздел “Мой статус” в Connect Asia:
o	отображение текущего статуса;
o	дата окончания;
o	преимущества VIP/PRO;
o	кнопки покупки/продления.
•	Отображение VIP/PRO в Space (бейджи, подсказки).
4. Применение преимуществ в токеномике и рефералах
•	В Referral Service:
o	внедрить разную глубину и коэффициенты начислений для BASE, VIP, PRO.
•	В Token Service:
o	разрешить VIP/PRO тратить Points на действия, недоступные BASE.
•	В RF и Quest:
o	помечать операции, доступные только VIP/PRO.
5. Маркетинговые и UX-элементы
•	Бейджи VIP/PRO в ленте, профилях, списках пользователей.
•	Информационные блоки: “Почему стоит оформить VIP?”, “Что даёт PRO?”.
•	Автоматические уведомления о скором окончании статуса.

Результат:
•	Экосистема получает прозрачную, стабильную монетизацию, полностью совместимую с философией открытого доступа к контенту (никакого paywall).
•	Пользователи получают сильную мотивацию повышать статус для:
o	расширения реферальной сети,
o	получения большего числа Points,
o	доступа к ваучерам и премиальным квестам,
o	получения дополнительных ролей и инструментов в экосистеме.
•	Платформа начинает приносить регулярный доход (VIP-подписки) и премиальный доход (PRO-сертификация).
•	Отсутствие платного контента сохраняет привлекательность модулей Atlas, Pulse, Blog — они остаются бесплатными для всех, что позволяет быстрее наращивать аудиторию.

## Этап 3.5: Единый пользовательский дашборд (слияние профиля и кабинета).

•	Цель: Объединить разбросанные по разным модулям показатели пользователя (социальные, реферальные, финансовые) в одном месте – сделать единый профиль/кабинет.
•	Задействованные компоненты: Фронтенд Space/Connect (интеграция UI); возможно, специальный BFF (Backend-for-Frontend) для профиля; API сбор данных из разных сервисов.
•	Вовлечённые агенты: ИИ-фронтенд-разработчик, ИИ-бэкенд-разработчик (Profile BFF), ИИ-архитектор UX, ИИ-тестировщик.
•	Задачи:
o	Объединить интерфейс: решить, где будет единый профиль. Можно встроить блоки Connect (балансы, рефералы) прямо в страницу профиля Space. Либо наоборот, сделать раздел Connect основным кабинетом и отобразить там социальные данные (например, число постов, подписчиков). Выбрать оптимальный путь с точки зрения UX.
o	Если требуется, реализовать профильный BFF: создать легковесный сервис, который по запросу к /api/profile/{userId} собирает данные с разных микросервисов – баланс Points/Tokens (Token Service), список достижений/NFT (Quest/Token), реферальную статистику (Referral), социальные показатели (Space: число друзей, постов) – и возвращает одним JSON. Это позволит фронтенду отрисовать всю панораму за один запрос.
o	Обновить фронтенд профильной страницы: отобразить все ранее разрозненные данные. Например, в Space-профиле добавить блок “Мои токены и поинты” (суммы с Token Service), “Реферальные достижения” (сколько пригласил, бонусы – из Referral), “Коллекция NFT” (список бейджей), “Уровень и статус” (VIP/PRO, возможно баллы опыта). Сделать интерфейс визуально привлекательным, с графиками или прогресс-барами, чтобы поощрять пользователя улучшать свои показатели.
•	Результат: Пользователь, зайдя в свой профиль, видит полную картину своей активности и вознаграждений. Например: “Вы - PRO-пользователь, ваш баланс: 1500 Points, 50 G2A; приглашено друзей: 5; выполнено квестов: 3 (бейджи: 2); написано постов: 10, читателей: 100” и т.п. Это повышает вовлечённость – все мотивационные метрики на виду. Раздел Connect как отдельное приложение может стать не нужен (или остаётся для детальных финансовых операций), главное – информация консолидирована. Такой дашборд к финальному релизу подчеркнёт ценность участия в экосистеме.

## Этап 3.6: Расширенная аналитика и мониторинг системы.

•	Цель: Внедрить масштабируемую систему сбора метрик и логирования, чтобы отслеживать использование платформы и её техническое состояние в режиме реального времени.
•	Задействованные компоненты: Сбор логов (Cloudflare Logs, микросервисы); хранилище аналитики (например, ClickHouse); дашборды метрик (Grafana/Supabase); мониторинг инфраструктуры (Prometheus/Sentry).
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-аналитик данных, ИИ-специалист по безопасности.
•	Задачи:
o	Настроить централизованный сбор событий и логов: подключить экспорт логов Cloudflare (для запросов CDN/WAF) во внутреннее хранилище, настроить микросервисы на отправку структурированных лог-сообщений и событий действий пользователя (например, “user_registered”, “post_created”, “token_purchase”) в очередь или лог-сервис.
o	Развернуть ClickHouse или аналог для аналитики: он позволит хранить большие объёмы данных и быстро считать агрегаты. Импортировать туда собранные логи и события (например, через ежедневный batch или стриминг). Создать базовые запросы/материализованные представления: количество DAU/MAU, конверсия рефералов, популярность разделов (сколько просмотров Blog vs Pulse), распределение по странам и т.п.
o	Визуализировать метрики: поднять Grafana или использовать готовую панель. Создать дашборды: график прироста пользователей, активность по дням, выполнение квестов, объем транзакций G2A и т.д. Настроить алерты на ключевые показатели (например, падение ежедневной активности ниже X).
o	Мониторинг технического здоровья: внедрить Prometheus для сбора метрик производительности (CPU, RAM каждого сервиса, время ответа API). Настроить оповещения об аномалиях: высокий response time Guru Service, ошибки 5xx на Auth Service, переполнение памяти и т.п. Также интегрировать Sentry или аналог для отслеживания исключений в фронтендах и бэкендах – чтобы сразу видеть баги у пользователей.
•	Результат: Команда получает глубокую прозримость в работу системы. На дашбордах в реальном времени видны ключевые метрики использования (сколько людей сейчас на платформе, какие разделы самые востребованные), а также состояние серверов. Это позволяет быстро реагировать на проблемы (например, если выросло время ответа – масштабировать сервис) и принимать продуктовые решения на основе данных. К финальному релизу платформа будет не черным ящиком, а управляемым по данным продуктом.

## Этап 3.7: Интеграция внешних сервисов и AI-функций.

•	Цель: Обогатить возможности платформы, подключив сторонние API (для карт и маркетинга) и первые элементы искусственного интеллекта для улучшения пользовательского опыта.
•	Задействованные компоненты: Геокодинг API (Google Maps или OSM); почтовый сервис (SendGrid/Mailchimp); Telegram-боты; AI-модели (OpenAI API) для чатов/модерации.
•	Вовлечённые агенты: ИИ-архитектор, ИИ-бэкенд-разработчик (интеграции), ИИ-фронтенд-разработчик, ИИ-специалист по AI.
•	Задачи:
o	Геосервисы: внедрить дополнительные данные для Atlas/Guru. Например, использовать Google Places API для подгрузки рейтингов или отзывов по локациям, либо OSM Overpass для нахождения объектов по координатам. Добавить в Guru детализацию: при клике на неучтённую локацию – запрашивать внешние данные (как опция). Также настроить геокодер (Google Geocoding API) как fallback при сохранении нового места, если Atlas не знает координаты.
o	Маркетинговые интеграции: подключить сервис e-mail рассылок (например, SendGrid) для автоматических писем: приветственное письмо новым пользователям, дайджест лучших статей недели для зарегистрированных. Также реализовать Telegram-бота, уведомляющего о новых событиях или личных сообщениях – для этого использовать Telegram API и webhook из Notification Service (который может быть выделен или встроен в Space).
o	AI-функции: добавить экспериментальные возможности на базе ML. Например, “AI-гид” в Guru – чат-бот, который знает об объектах Atlas и может отвечать на вопросы туриста в режиме диалога (реализовать вызов OpenAI API с передачей описаний ближайших мест). Или автоматическая модерация контента: настроить отправку текстов постов/комментариев на проверку модели (через OpenAI или специализированный сервис) на наличие оскорблений или запрещенного контента.
•	Результат: Платформа становится умнее и более связанной с внешним миром. Пользователи получают ценность от AI (быстрые ответы и персонализированные рекомендации), а интеграции улучшают качество данных (актуальные отзывы, более точные карты) и удержание (email-рассылки напоминают вернуться). Эти дополнительные возможности не являются центральными для релиза, но служат приятными улучшениями, повышающими конкурентоспособность Go2Asia.

## Этап 3.8: Выпуск обновления (конец фазы 3) и контроль переходного периода.

•	Цель: Развернуть все новые функции, обеспечив их стабильное включение без ущерба для имеющихся, и подготовиться к фазе оптимизации.
•	Задействованные компоненты: Все новые сервисы (Gateway, обновлённый Token, DAO контракт, др.) и фронтенды; система feature-flags для плавного включения.
•	Вовлечённые агенты: ИИ-менеджер проекта, ИИ-DevOps-инженер, ИИ-тестировщик, ИИ-специалист по безопасности.
•	Задачи:
o	Постепенное включение токеномики: сначала запустить Token Service с поддержкой G2A в “тихом режиме” – собирать данные, но не открывать пользователям интерфейс обмена. Проверить корректность взаимодействия с блокчейном на тестовой сети (TON testnet). После проверки – включить функциональность конвертации для ограниченной группы, затем для всех.
•	Результат: Третья фаза успешно завершается. Платформа Go2Asia теперь не только контентный и социальный сервис, но и экономическая система: пользовательская активность конвертируется в токены, которые имеют ценность вне экосистемы. Партнёры вовлечены экономически, сообщество – в управлении. Все сервисы интегрированы в единое пространство, а аналитика показывает рост активности благодаря новым стимулам. Система усложнилась, но благодаря модульности и поэтапному включению функций удалось избежать дестабилизации. Впереди – заключительная фаза оптимизации и масштабирования.

---

# Фаза 4: Оптимизация и устойчивость (масштабирование, производительность, финальная полировка)

Цель фазы 4: Обеспечить долгосрочную масштабируемость, высокую производительность и отказоустойчивость экосистемы перед выходом на массовую аудиторию. После множества введённых функций важно устранить технический долг, оптимизировать узкие места и подготовить инфраструктуру к росту нагрузки в разы. Также проводится финальная шлифовка системы, закрытие уязвимостей и подготовка документации для релиза.

## Этап 4.1: Вынос нагруженных задач на Edge-функции.

•	Цель: Сократить задержки и нагрузку на центральные серверы, передав часть работы глобальной распределённой сети (Cloudflare Workers и аналоги).
•	Задействованные компоненты: Cloudflare Workers; самые частые API-запросы (статистика, агрегаторы); KV-хранилище Cloudflare для кэша.
•	Вовлечённые агенты: ИИ-архитектор, ИИ-бэкенд-разработчик, ИИ-DevOps-инженер.
•	Задачи:
o	Проанализировать метрики (из фазы 3 аналитики) и определить самые часто вызываемые и “тяжёлые” запросы. Например, агрегация Guru по нескольким сервисам, геокодирование адресов, проверка промо-кодов, массовая рассылка уведомлений.
o	Реализовать эти функции на Edge: написать Cloudflare Worker-скрипты, которые могут выполнить часть логики автономно. Например, Worker, принимающий запрос /api/guru/nearby – вместо маршрутизации на центральный Guru Service – сам хранит кэш популярных мест и возвращает сразу результат, обновляя кэш фоново. Или Worker для проверки JWT: при вызове защищённых маршрутов Space, он на Edge валидирует токен и решает пускать ли дальше.
o	Использовать распределённое KV-хранилище Cloudflare для хранения часто используемых справочников (например, список популярных городов Atlas или настроек) – чтобы Edge-скрипты могли отвечать не обращаясь к основной базе. Обновить CI/CD, чтобы деплой Workers был автоматическим.
•	Результат: Существенная доля трафика (цель – 30-40%) теперь обслуживается на уровне CDN/Edge без обращения к центральным серверам. Это резко повышает быстродействие для пользователей по всему миру (ответ возвращается с ближайшего узла) и разгружает основные сервисы, позволяя им масштабироваться для более сложных задач. Латентность снижается, опыт пользователя улучшается, а инфраструктура готова выдерживать географически распределённые нагрузки.

## Этап 4.2: Масштабирование базы данных и данных.

•	Цель: Подготовить хранение данных к росту – перейти на масштабируемые решения для баз данных, кэшей, поисковых индексов.
•	Задействованные компоненты: Основные БД (Postgres) – миграция в Neon или облако; Redis для кэша; кластер Meilisearch/Elastic.
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-архитектор БД, ИИ-бэкенд-разработчик.
•	Задачи:
o	Мигрировать PostgreSQL на масштабируемую архитектуру: внедрить Neon (serverless Postgres) или аналог. Neon отделяет хранение от вычислений и автоматически масштабируется по нагрузке. Выполнить перенос данных Atlas, Content, Space и др. на Neon с минимальным простоем. Настроить репликацию и бэкап: Neon позволяет создавать read-replica и хранить снимки, что добавляет отказоустойчивости.
o	Внедрить Redis: развернуть Redis-кластер для задач кэширования (например, часто запрашиваемые топ-списки, сессии) и для очередей (если нужно очень быстрое бронирование, хотя основная очередь уже есть). Обновить сервисы (например, Guru Service, Space feed) – чтобы они использовали Redis как внешнее кеш-хранилище вместо собственной памяти, что позволит масштабировать их горизонтально без потери кэша.
o	Масштабировать поисковой движок: если используется Meilisearch, запустить его в кластерном режиме (шардирование индексов по категориям или регионам); если Elastic – настроить 2-3 узла. Перестроить индексы при необходимости, чтобы поиск оставался быстрым даже при увеличении данных (например, 100k+ статей, 1M+ постов).
•	Результат: Хранение данных выходит на enterprise-уровень. Основные базы теперь автоматически масштабируются и выдерживают скачки нагрузки (например, пик регистраций или контентного трафика). Критичные данные защищены репликацией и бэкапами – риск потери сведён к минимуму. Кэш и поиск тоже масштабированы: пользователи не заметят замедления даже при 10-кратном росте контента. Сервис готов к массовому использованию.

## Этап 4.3: Оптимизация отдачи медиа и фронтенда.

•	Цель: Ускорить загрузку тяжёлого контента (изображений, видео) и обеспечить максимальную производительность фронтенд-приложения даже при слабом интернете.
•	Задействованные компоненты: Cloudflare Image Resizing; механизмы кэширования статики; оптимизация бандлов фронта.
•	Вовлечённые агенты: ИИ-DevOps-инженер, ИИ-фронтенд-разработчик.
•	Задачи:
o	Подключить Cloudflare Image Resizing: изменить URL выдачи изображений (аватаров, фото мест и т.д.) на специальные проксирующие, которые автоматически трансформируют картинки под нужные размеры/форматы (например, WebP). Обновить фронтенд, чтобы запрашивать изображения с параметрами (width, format) – таким образом, пользователям будут отправляться уже оптимизированные файлы, что ускорит загрузку и снизит трафик.
o	Установить длинное кэширование для статики: настроить заголовки CDN так, чтобы статические ресурсы (JS/CSS, а также загруженные медиа) кешировались на клиентах и на CDN как можно дольше (например, 30-60 дней)[89][90]. Реализовать версионирование файлов (например, хэш в имени файла) при сборке, чтобы при обновлении деплоя не было конфликтов кэша. Добавить вызов Cloudflare Cache Purge в CI/CD, чтобы при релизе сбрасывать старый кэш критических ресурсов.
o	Минимизировать и разделить бандлы фронтенда: пересмотреть сборку Bolt-модулей – убедиться, что включён treeshaking, code splitting. Возможно, вынести общие зависимости (React, SDK) в App Shell, чтобы не дублировать их в каждом микрофронтенде. Проверить загрузку на Lighthouse/PageSpeed и устранить выявленные проблемы (например, слишком большие initial chunks).
•	Результат: Легковесность фронтенда достигает максимума. Визуальный контент загружается быстро и адаптивно под устройство пользователя (больше не нужно скачивать 5МБ фото на мобильном – CDN сам отдаст уменьшенную версию). После первого визита почти все ресурсы берутся из кэша, приложение мгновенно открывается и работает даже офлайн. Это особенно важно для мобильных пользователей в странах ЮВА, где связь может быть медленной – оптимизации обеспечивают комфортный UX и в этих условиях.

## Этап 4.4: Усиление безопасности на уровне Edge и сервиса.

•	Цель: Закрыть оставшиеся возможности для злоупотреблений и атак, внедрив многоуровневую защиту (на Edge, в коде сервисов, в политиках доступа).
•	Задействованные компоненты: Cloudflare Workers (авторизация на Edge); правила Firewall/Rate Limiting; аудит кода (безопасность).
•	Вовлечённые агенты: ИИ-специалист по безопасности, ИИ-DevOps-инженер, ИИ-бэкенд-разработчик.
•	Задачи:
o	Реализовать JWT-авторизацию на Edge: теперь Cloudflare Workers могут выполнять проверку токена Clerk до передачи запроса на бэкенд. Настроить Worker на маршрутах /api/ так, чтобы он отвергал запросы без валидной аутентификации сразу (для приватных ресурсов). Аналогично – проверка ролей на Edge: например, доступ к /api/admin/* разрешён только для JWT с ролью admin (Edge-скрипт декодирует JWT, проверяет claim). Это разгружает сервисы и добавляет слой безопасности.
o	Настроить Rate Limiting и Bot Management: использовать возможности Cloudflare Firewall – ограничить частоту вызовов API для одного IP/токена (например, не более 10 запросов в секунду на пользователя). Включить защиту от ботов: для подозрительного трафика (отсутствие браузерных признаков, известные вредоносные IP) – выдавать CAPTCHA или блокировать. Добавить правила блокировки запросов с аномальной активностью (спам регистраций, brute force на логин).
o	Провести полный аудит безопасности: AI-агент безопасности просматривает код критических модулей (Auth, Wallet Gateway, Space) на предмет уязвимостей (SQL-инъекции, XSS, неправильная валидация входных данных). Проверяет настройки Cloudflare, конфигурацию CORS, хранения секретов. Результаты аудита – исправить найденное: внедрить Content Security Policy строже, убрать экспонирование лишних данных в API, настроить HTTP Security Headers, и т.д.
•	Результат: Платформа становится значительно более защищённой. Атакующему теперь крайне сложно перегрузить сервисы (Edge отсекает злоупотребления), украсть данные (JWT проверяется и минимизируется его срок, все коммуникации HTTPS+HSTS), или получить несанкционированный доступ (роли проверяются на внешнем уровне). При этом реальные пользователи не ощущают этих мер (разве что получат CAPTCHA, если ведут себя подозрительно). Команда может уверенно заявлять о серьёзном отношении к безопасности, что важно перед большим релизом.

## Этап 4.5: Рефакторинг и устранение технического долга.

•	Цель: Переписать временные решения, принятые на фазах MVP, на более стабильные и масштабируемые, а также унифицировать кодовую базу.
•	Задействованные компоненты: Content Service (возможный раздел), конфиги микросервисов, оптимизация запросов БД, обновление контрактов типов.
•	Вовлечённые агенты: ИИ-архитектор, ИИ-бэкенд-разработчик, ИИ-фронтенд-разработчик.
•	Задачи:
o	Заменить оставшиеся временные заглушки: например, если Blog до сих пор для каких-то данных обращался во внешний CMS (Contentful) – перенести эти данные во внутренний Content Service и отключить внешнюю зависимость. Если какие-то параметры были жестко прописаны (hardcode) – вынести их в конфигурационные файлы (.env, Config Service) для гибкости.
o	Оптимизировать тяжелые запросы: проанализировать slow-query лог базы. Добавить индексы для часто используемых фильтров (например, индекс по дате в таблице событий, по тегам в статьях). Переписать неэффективные JOIN’ы или разделить слишком большие запросы на несколько более простых. Использовать Redis как кэш для результатов тяжёлых агрегатов (например, топ-10 популярных постов дня).
o	Повысить устойчивость микросервисов: внедрить шаблоны Circuit Breaker и retry. Например, если Referral Service пытается вызвать Token Service и тот не отвечает – сразу возвращать ответ с ошибкой/кэшем, не дожидаясь долгого таймаута (тем самым изолируя сбой). Добавить повторные попытки с увеличением интервала для фоновых задач (например, повторно отправить вебхук через 5, 15, 30 секунд при неудаче).
o	Обновить контрактную типизацию: сверить все сервисы и фронтенды с репозиторием @go2asia/contracts – обновить интерфейсы данных, гарантировать что и фронт, и бэк используют одну версию структур. Это предотвратит несоответствия при обмене данными. Ввести линтинги/типовые тесты, чтобы при изменении контрактов тесты падали, сигнализируя о необходимости синхронного обновления всех частей.
•	Результат: Кодовая база очищена от “долгов молодости”. Все модули работают на актуальных архитектурных решениях, заложенных в начале, но без костылей, которые могли накопиться в спешке MVP. Запросы к базе быстры, нет лишних обращений; система выдерживает отказ отдельных сервисов без полной деградации. Поддержка и развитие кода после релиза станет проще благодаря единообразию и чистоте архитектуры.

## Этап 4.6: Финальное тестирование, аудит и документация.

•	Цель: Провести всестороннее тестирование готовой системы, устранить последние баги, подготовить все необходимые документы и материалы перед публичным релизом.
•	Задействованные компоненты: Вся экосистема; тестовые сценарии; пользовательская и техническая документация.
•	Вовлечённые агенты: ИИ-тестировщик, ИИ-специалист по безопасности (заключительный аудит), ИИ-технический писатель.
•	Задачи:
o	Регрессионное тестирование: QA-агент проходит все ранее описанные сценарии, а также разрабатывает новые – имитируя поведение реальных пользователей. Проверяется корректность начисления токенов, робастность офлайн-режимов, консистентность данных при массовых операциях. Также проводятся penetration tests (автоматизированные проверки уязвимостей) и нагрузочные тесты на пиковые сценарии. Все обнаруженные баги классифицируются и исправляются.
o	Окончательный аудит безопасности: провести ещё один полный цикл анализа, убедиться, что предыдущие рекомендации выполнены. Проверить смарт-контракты (по возможности аудировать внешним сервисом или AI) – особенно на отсутствие уязвимостей, которые могут привести к потере средств. Настроить bug-bounty программу (при желании) для сторонних исследователей.
o	Подготовка документации: технический писатель собирает всю информацию в единый комплект документов. Создаются: User Guide (инструкция для пользователей по всем функциям), Developer Documentation (описание архитектуры, модулей, API – для будущих разработчиков/контрибьюторов), API Reference (детально по всем эндпоинтам REST/GraphQL), Deployment Guide (как разворачивать инфраструктуру), Security Whitepaper (описание модели безопасности и токеномики для партнёров). Документы оформляются в репозитории (например, в markdown в папке docs/) и частично на веб-сайте.
•	Результат: Платформа Go2Asia проходит финальную проверку и считается готовой к публичному финальному релизу. Качество подтверждено: производительность оптимальна, безопасность на высоком уровне, функциональность тщательно протестирована. Документация позволяет команде и сообществу понимать устройство системы и взаимодействовать с ней. Теперь Go2Asia – зрелый продукт, готовый к масштабному запуску и дальнейшему сопровождению.

## Итог фазы 4

После всех этих оптимизаций экосистема Go2Asia становится по-настоящему эластичной и устойчивой. PWA-фронтенд быстро работает даже при слабом соединении или на устаревших устройствах; значительная часть запросов обслуживается кэшем или Edge-функциями, обеспечивая мгновенный отклик. Бэкенд распределён и отказоустойчив – выход из строя отдельного сервиса не парализует систему, данные защищены (кластеризация БД, резервные копии), а рост нагрузки автоматически обрабатывается за счёт серверлесс-скейлинга. Таким образом, платформа полностью готова к приёму большой аудитории и дальнейшему развитию без кардинальных переделок архитектуры.

---

# Зависимости между этапами и фазами

•	Фаза 0 является базой для всех последующих работ – монорепозиторий, CI/CD, SSO и облачная инфраструктура должны быть настроены до начала реализации функционала. Без завершения этапов фазы 0 невозможна эффективная разработка модулей (например, мультиагентная команда не сможет параллельно работать без структуры репозитория и CI).
•	Внутри фазы 1 существует зависимость фронтенда от бэкенда: контентные модули Atlas, Pulse, Blog разрабатываются параллельно с Content API Service и Auth Service. Однако для полноценного тестирования Atlas/Pulse/Blog необходим функционирующий Content API, поэтому его разработка (этап 1.6) должна как минимум частично завершиться прежде, чем фронтенды интегрируются. Аналогично, Referral Service и Token Service (этапы 1.7–1.8) должны быть готовы к моменту запуска MVP, чтобы при регистрации начислялись поинты. Тем не менее, фронтенд-модули могут быть разработаны с заглушками и затем подключены к готовым API.
•	Фаза 2 – многие новые модули могут создаваться независимо друг от друга (соцсеть, квесты, каталог партнёров и др.), что мультиагентная команда и делает параллельно. Тем не менее, некоторые зависимости присутствуют: Space (соцсеть) и Quest используют Token Service (для начисления баллов) – он был реализован в фазе 1, но в фазе 2 может потребовать расширения. Guru модуль зависит от наличия данных Atlas/Pulse/Rielt – эти сервисы либо были на MVP (Atlas/Pulse), либо запускаются одновременно (Rielt). Поэтому Guru Service делается ближе к концу фазы 2, когда основные источники данных уже работают. Также модуль Connect (этап 2.2) опирается на данные Referral/Token, которые появились в фазе 1, но в фазе 2 они обогащаются новыми полями (PRO-партнёры и т.д.).
•	Фаза 3 требует полностью функционирующей социальной и контентной базы (фазы 1–2) перед добавлением экономики. Например, ввод токена G2A и NFT имеет смысл только когда уже есть пользователи, накапливающие Points и достижения. Таким образом, сначала запускаются механики вовлечения (рефералы, квесты, социальные связи), и лишь затем – их монетизация и ценные активы. Блокчейн-интеграция зависит от готовности Token Service (фаза 1–2) и Wallet Gateway следует внедрять после того, как внутренние операции Points налажены. DAO-голосования тоже требуют, чтобы достаточное число пользователей получили токены (т.е. токеномика должна заработать).
•	Фаза 4 зависит от завершения всех функциональных нововведений (фазы 1–3). Оптимизации и масштабирование проводятся на основе реальной нагрузки и профиля использования, полученных ранее. Например, вынести в Edge можно только поняв, какие запросы самые частые (по аналитике фазы 3). Масштабировать БД имеет смысл, когда объём данных уже значителен (к концу фазы 3). Таким образом, никакие оптимизации фазы 4 не блокируют разработку функционала, они выполняются после или параллельно с финальными тестами. Некоторые улучшения (например, базовый кэш, индексы) можно делать по ходу, но основная работа – после интеграции всех функций.

# MVP и пост-MVP этапы

•	Этапы MVP (Minimum Viable Product): это фаза 1 (после подготовки фундамента в фазе 0). MVP-версия платформы включает три контентных раздела – Atlas, Pulse, Blog – и базовые сервисы: аутентификация, единый контентный API, реферальная программа и система Points. Такие возможности достаточны, чтобы продемонстрировать ядро продукта: пользователи могут получать полезный контент и первые бонусы за активность. В MVP-состоянии социальные интерактивные функции ограничены (комментарии через внешний сервис, нет внутренней соцсети), экономика упрощена (только очки лояльности, без реального токена). Главное – платформа уже работает как целостный продукт, хоть и в урезанном виде. Это позволяет начать привлекать ранних пользователей и получать обратную связь.
•	Этапы Post-MVP: фазы 2, 3 и 4 относятся к развитию после MVP. В фазе 2 вводятся ключевые модули взаимодействия и роста – полноценная соцсеть (Space), геймификация (Quest), партнерские программы (RF, Connect), агрегатор Guru. Эти функции значительно расширяют продукт сверх MVP, превращая его из просто информационного ресурса в активную экосистему. Фаза 3 добавляет экономику и интеграцию: собственный токен G2A, NFT-награды, платные сервисы, аналитику – что выходит за рамки минимально необходимого и делает Go2Asia более инновационной и способной к монетизации. Фаза 4 – оптимизация и масштабирование – вообще не добавляет новых пользовательских фич, но крайне важна для качественного финального релиза (без неё риски падения и проблем при росте высоки). Таким образом, MVP охватывает ограниченный функционал (контент + базовые соц. механики), а финальный релиз (конец фазы 4) включает весь комплекс возможностей: социальная сеть, партнёрская программа, токеномика, DAO, и т.д., с обеспечением производительности и безопасности на уровне производства. Согласно плану, Atlas как критически важный контентный сервис был реализован к MVP, тогда как более сложные модули (Pulse, Blog полные версии, Space, RF, Connect, Quest) запускаются постепенно на пост-MVP этапах. Это соответствует приоритету сначала дать ценность (контент) пользователям, а затем наращивать вокруг неё взаимодействия и экономику.

# Особенности мультиагентной модели разработки

Разработка Go2Asia осуществлялась с применением мультиагентного подхода, что существенно повлияло на организацию работы и эффективность выполнения плана. Главный ИИ-оркестратор исполнял роль менеджера проекта: он разбивал общие цели на описанные выше этапы, назначал задания профильным агентам и следил за их выполнением. Такое разграничение позволило одновременно трудиться над разными компонентами без контекстных конфликтов: каждый субагент фокусировался на своей области (frontend, backend, DevOps и пр.), не перегружаясь всей системой целиком.
В частности, следующие роли агентов были задействованы на разных этапах: 
- ИИ-DevOps-инженер – с самого начала (фаза 0) настроил репозиторий и инфраструктуру CI/CD, а затем на каждой фазе помогал с деплоем новых сервисов, конфигурировал мониторинг и масштабирование. Этот агент также совместно с ИИ-бэкендами выносил функции на Edge и настраивал базы данных на фазе 4. 
- ИИ-фронтенд-разработчик(и) – занимались созданием PWA-модулей Atlas, Pulse, Blog на фазе 1, а затем Space, RF, Connect, Quest, Guru на фазе 2, включая интеграцию их в общую оболочку. Благодаря этому, фронтенд для каждого модуля разрабатывался параллельно разными агентами, но в едином стиле и с общей дизайн-системой (оркестратор и аналитик следили за согласованностью UX). Агент фронтенда уделял внимание оптимизации UX (например, mobile-first, офлайн-cache) согласно требованиям, генерируя современный и эффективный код UI. 
- ИИ-бэкенд-разработчики – параллельно создавали микросервисы: один агент мог работать над Content и Auth на фазе 1, другой – над Referral и Token, далее на фазе 2 отдельные агенты взяли Space Service, Quest Service, Guru Service, Rielt Service и т.д. Оркестратор координировал их так, чтобы контракты API между сервисами и фронтом соблюдались. Каждый бэкенд-агент генерировал код сервисов, следуя заданной архитектуре (REST API, интеграция с очередью, безопасность), что обеспечило высокое качество и согласованность server-side логики. 
- ИИ-аналитик – помогал уточнять требования перед началом разработки каждого крупного модуля. Например, перед фазой 2 описывал user story для Space (посты, друзья, группы) и Quest (сценарии квестов), чтобы разработчики чётко понимали функциональность. Аналитик также помог сформулировать токеномику (фаза 3) и правила монетизации на понятном для разработки языке. 
- ИИ-архитектор – контролировал общую техническую целостность решений: проектировал схемы БД, продумывал взаимодействие сервисов, выбирал технологии (например, решение использовать Cloudflare Workers на фазе 4). Он участвовал во всех фазах, периодически пересматривая архитектуру при добавлении новых компонентов, чтобы система оставалась модульной и масштабируемой. 
- ИИ-тестировщик (QA) – подключался к концу каждой фазы, создавая и выполняя планы тестирования. Например, на фазе 1 он написал юнит-тесты для Content API, провёл ручное тестирование фронтенда Atlas/Pulse/Blog на разных устройствах. На фазе 2 – тестировал сценарии соцсети (добавление в друзья, уведомления), корректность начисления поинтов при квестах, и т.д. На заключительной фазе QA-агент провёл регрессионное тестирование и нагрузочные прогоны, имитирующие рост нагрузки. Все обнаруженные баги он передавал обратно команде через оркестратора для исправления. 
- ИИ-специалист по безопасности – привлекался на этапах крупных релизов (конец фазы 3 и 4) для аудита. Он проверял код смарт-контрактов, конфигурации Cloudflare, поискал SQL-инъекции и XSS, проверил защиту API. Отчёт этого агента позволил устранить критичные уязвимости перед финальным запуском. 
- ИИ-технический писатель – в финале проекта (фаза 4) задокументировал систему: сформировал понятные README, руководства по деплою, описал архитектуру модулей, подготовил user manual для конечных пользователей и API-справочники для разработчиков внешних интеграций. Это гарантировало, что знания о системе не потеряются, и упростит обучение новых членов команды или открытие проекта сообществу.
Благодаря мультиагентному подходу, работа шла итеративно и параллельно. Оркестратор, получая результаты от субагентов, оперативно их объединял и при необходимости корректировал план дальнейших шагов. Например, если фронтенд-агент Blog закончил раньше, оркестратор мог перенаправить его к помощи агенту Space или заняться оптимизацией. Каждое решение сверялось с общей картиной, которую держал в фокусе менеджер-агент. Коммуникация между агентами происходила на русском языке, что упрощало обсуждение нюансов для команды.
В итоге мультиагентная модель доказала свою эффективность в проекте Go2Asia: разработка сложной экосистемы прошла организованно, без длительных узких мест, с высоким качеством на каждом участке. Каждый агент внёс экспертный вклад в свою часть, а скоординированная работа под управлением оркестратора позволила достичь целей в указанные этапы и подготовить платформу к финальному релизу в оптимальные сроки.

